<div class="single">
  <!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.17-DEV" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <link rel='stylesheet' href='//fonts.googleapis.com/css?family=Open+Sans|Marcellus+SC'>
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.min.css">
    <link rel="stylesheet" href="http://blog.dgraph.io/css/styles.css">
    <link rel="stylesheet" href="http://blog.dgraph.io/css/custom.css">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="http://blog.dgraph.io//index.xml">

    
    <title>Can it really scale? - Dgraph Blog</title>
    <meta property='og:title' content="Can it really scale? - Dgraph Blog">
    <meta property="og:type" content="article">
    

    <meta property="og:url" content="http://blog.dgraph.io/post/performance-throughput-latency/">
    
    <meta property="og:image" content="http://blog.dgraph.io/images/lander.jpg">

  </head>

  <body>

    <header class="site">
      <div class="title"><a href="http://blog.dgraph.io/">Dgraph Blog</a></div>
    </header>

    <div class="container site">



  <div class="row">
    <div class="col-sm-12">

      <article class="single" itemscope="itemscope" itemtype="http://schema.org/Article">

        <meta itemprop="mainEntityOfPage"  itemType="https://schema.org/WebPage" content="http://blog.dgraph.io/"/>
        <meta itemprop="dateModified" content="2016-06-21T10:20:32&#43;05:30">
        <meta itemprop="headline" content="Can it really scale?">
        <meta itemprop="description" content="In this post, we’ll look at how Dgraph performs on varying the number of nodes in the cluster, specs of the machine and load on the server to answer the ultimate question: Can it really scale?
">
        <meta itemprop="url" content="http://blog.dgraph.io/post/performance-throughput-latency/">
        <div itemprop="image" itemscope itemtype="https://schema.org/ImageObject">
          <meta itemprop="url" content="http://blog.dgraph.io/images/lander.jpg" />
          <meta itemprop="width" content="800">
          <meta itemprop="height" content="800">
        </div>
        <div itemprop="publisher" itemscope itemtype="https://schema.org/Organization">
          <div itemprop="logo" itemscope itemtype="https://schema.org/ImageObject">
            <meta itemprop="url" content="http://blog.dgraph.io/images/logo.jpg">
            <meta itemprop="width" content="100">
            <meta itemprop="height" content="100">
          </div>
          <meta itemprop="name" content="Dgraph Blog">
        </div>
        <div itemprop="author" itemscope itemtype="https://schema.org/Person">
          <meta itemprop="name" content="">
        </div>

        <div class="image" style="background-image: url(http://blog.dgraph.io/images/lander.jpg);"></div>

        <header class="article-header">
          <time itemprop="datePublished" pubdate="pubdate" datetime="2016-06-21T10:20:32&#43;05:30">Tue, Jun 21, 2016</time>
          <h1 class="article-title">Can it really scale?</h1>
        </header>

        <div class="article-body" itemprop="articleBody">
          

<p>In this post, we’ll look at how Dgraph performs on varying the number of nodes in the cluster, specs of the machine and load on the server to answer the ultimate question: <em>Can it really scale?</em></p>

<h2 id="the-dataset">The Dataset</h2>

<p><a href="https://en.wikipedia.org/wiki/Freebase">Freebase</a> is an online collection of structured data which includes contributions from many sources including individual and user-generated contributions.
Currently, it has <a href="https://developers.google.com/freebase/">1.9 Billion RDF N-Triples</a> worth 250GB of uncompressed data.
On top of that, this dataset is over 95% accurate with a complex and rich real world schema.
It is an ideal data set to test the performance of Dgraph.
We decided not to use the entire data set as it wasn&rsquo;t necessary for our goal here.</p>

<p>Given our love for movies, we narrowed it down to the film data.
We ran some scripts and filtered in the movie data only.
All the data and scripts are present in <a href="https://github.com/dgraph-io/benchmarks/tree/master/data">our benchmarks repository</a>.
There are two million nodes, which represent directors, actors, films and all the other objects in the database.
Moreover, 21 million edges (including 4M edges for names) are representing the relationships between actors, films, directors and all the other nodes in the database.</p>

<hr />

<p>Some interesting information about this data:</p>

<pre><code># film.film --{film.film.starring}--&gt; [mediator] --{film.performance.actor}--&gt; film.actor
# Film --&gt; Mediator
$ zgrep &quot;&lt;film.film.starring&gt;&quot; rdf-films.gz | wc -l
1397647

# Mediator --&gt; Actor
$ zgrep &quot;&lt;film.performance.actor&gt;&quot; rdf-films.gz | wc -l
1396420

# Film --&gt; Director
$ zgrep &quot;&lt;film.film.directed_by&gt;&quot; rdf-films.gz | wc -l
242212

# Director --&gt; Film
$ zgrep &quot;&lt;film.director.film&gt;&quot; rdf-films.gz | wc -l
245274

# Film --&gt; Initial Release Date
$ zgrep &quot;&lt;film.film.initial_release_date&gt;&quot; rdf-films.gz | wc -l
240858

# Film --&gt; Genre
$ zgrep &quot;&lt;film.film.genre&gt;&quot; rdf-films.gz | wc -l
548152

# Genre --&gt; Film
$ zgrep &quot;&lt;film.film_genre.films_in_this_genre&gt;&quot; rdf-films.gz | wc -l
546698

# Generated language names from names freebase rdf data.
$ zcat langnames.gz | awk '{print $1}' | uniq | sort | uniq | wc -l
55

# Total number of countries.
$ zgrep &quot;&lt;film.film.country&gt;&quot; rdf-films.gz | awk '{print $3}' | uniq | sort | uniq | wc -l
304
</code></pre>

<hr />

<p>This data set contains information about ~480K actors, ~100K directors and ~240K films.
Some example of entries in the dataset are :</p>

<pre><code>&lt;m.0102j2vq&gt; &lt;film.actor.film&gt; &lt;m.011kyqsq&gt; .
&lt;m.0102xz6t&gt; &lt;film.performance.film&gt; &lt;m.0kv00q&gt; .
&lt;m.050llt&gt; &lt;type.object.name&gt; “Aishwarya Rai Bachchan”@hr .
&lt;m.0bxtg&gt; &lt;type.object.name&gt; “Tom Hanks”@es .
</code></pre>

<h2 id="terminology">Terminology</h2>

<ul>
<li>Throughput: Number of queries served by the server per second and received by the client</li>
<li>Latency: Difference between the time when the server received the request and the time it finished processing the request</li>
<li>95 percentile latency: The worst case latency which 95 percentage of users that query the database face</li>
<li>50 percentile latency: The worst case latency which half the users that query the database face</li>
</ul>

<h2 id="setup">Setup</h2>

<p>All the testing was done on GCE instances. Each machine had 30GB of SSD and at least 7.5 GB of RAM. The number of cores varied depending on the experiments performed.</p>

<p>The tests were run for 1-minute intervals during which all the parallel connections made requests to the database.
This was repeated ten times and throughput, mean latency, 95th percentile latency, 50th percentile latency were measured.
Note that for user-facing systems, measuring percentile latency is better than mean latency as the average can be skewed by outliers.</p>

<p>In a multi-node cluster set up, the queries were distributed among each node in a round-robin fashion.
Note that no single machine contains all the data to answer these queries, in a multi-node cluster.
They still have to communicate with each other to respond to the queries.</p>

<h2 id="variables">Variables</h2>

<p>The parameters that were varied were:</p>

<ol>
<li>A number of parallel connections to the database. In Go, this equated to the number of goroutines a client would have. Each goroutine would run in an infinite loop, querying the database via a blocking function.</li>
<li>Number of cores per server</li>
<li>Number of servers in the cluster</li>
</ol>

<p>This gave us an idea of what to expect from the system and would help in predicting the configuration required to handle a given load.</p>

<h2 id="queries">Queries</h2>

<p>We ran broadly 2 categories of queries.</p>

<ul>
<li>For each actor (478,936 actors), get their name, the films they acted in, and those films&rsquo; names.</li>
</ul>

<pre><code>{
  me ( _xid_ : XID ) {
    type.object.name.en
    film.actor.film {
      film.performance.film {
        type.object.name.en
      }
    }
  }
}
</code></pre>

<ul>
<li>For each director (90,063 directors), get their name, the films they directed, and names of all the genres of those films.</li>
</ul>

<pre><code>{
  me ( _xid_ : XID ) {
    type.object.name.en
    film.director.film {
      film.film.genre {
        type.object.name.en
      }
    }
  }
}
</code></pre>

<p>During each iteration, either an actor or a director category was chosen randomly.
Furthermore, for that category, an actor or director was chosen randomly; their <code>XID</code> filled in in the query template.</p>

<h2 id="performance">Performance</h2>

<p>Let us look at some graphs obtained by varying the machine specs and the number of nodes in the cluster under different loads.</p>

<h3 id="vary-the-number-of-cores-in-a-single-instance">Vary the number of cores in a single instance</h3>

<p><img src="/images/cores_thru.jpg" alt="Throughput on varying number of cores" /></p>

<p><img src="/images/cores_lat_50.jpg" alt="50 percentile latency on varying number of cores" /></p>

<p><img src="/images/cores_lat_95.jpg" alt="95 percentile latency on varying number of cores" /></p>

<p><img src="/images/cores_lat_mean.jpg" alt="mean latency on varying number of cores" /></p>

<ul>
<li>With the same number of cores, when we increase the number of connections, i.e. load on the system, the throughput as well as the latency increase.</li>
<li>Throughput increases till some point and then flattens out. This is the point where the computational capacity is being utilized almost fully.</li>
<li>As expected, the latency increases almost linearly with the number of connections.</li>
<li>When we increase the number of cores, the latency decreases and the throughput increases.</li>
</ul>

<h3 id="vary-number-of-instances">Vary number of instances</h3>

<p><img src="/images/topo_thru.jpg" alt="Throughput on varying number of instances" /></p>

<p><img src="/images/dist_lat_50.jpg" alt="50 percentile latency on varying number of instances" /></p>

<p><img src="/images/dist_lat_95.jpg" alt="95 percentile latency on varying number of instances" /></p>

<p><img src="/images/dist_lat_mean.jpg" alt="mean latency on varying number of instances" /></p>

<ul>
<li>When we increase the number of parallel connections, the throughput increases, but then flattens out. This is the point where the computational capacity is being utilized almost fully.</li>
<li>The latency increases almost linearly with the number of connections.</li>
<li>Latency in the case of a single instance is observed to be the equal to (or a bit lower than) that of distributed configurations as the former doesn’t require any network calls. However, as the number of requests/load increase, the cumulative computational power comes into play and overshadows the latency incurred due to network calls. Hence, the latency reduces in the distributed version under higher loads.</li>
<li>On comparing across the one, two and five node clusters, we can see that the latency, as well as the throughput, are better for configurations with a higher number of nodes, i.e., when there is more computational capacity at disposal. The throughput increases as we have greater computational power and can handle more queries.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>From the above experiments, we can see a relationship between the throughput, latency and the overall computational power of the cluster.
The graphs show that the throughput increases as the computational power increases.
Which can be achieved either by increasing the number of cores on each server or the number of nodes in the cluster.</p>

<p>The latency increases as the amount of load on the database increases.
However, the rate of the increase differs based on how much computational power we have available.</p>

<p>This experiment also shows that there is a limit on how much computational power a single node can have, and once we reach that limit, scaling horizontally is the right option.
Not only that, but it also proves that scaling horizontally improves the performance.
Hence, having more replicas, distributing the dataset optimally across machines are some factors which help in improving the throughput and reducing the latency that the users face.</p>

<p>Based on this experiment, our recommendation for running Dgraph would be:</p>

<ul>
<li>Use as many cores as possible</li>
<li>Have the servers geographically close-by so that network latency is reduced</li>
<li>Distribute the data among servers and query them in a round-robin fashion for greater throughput</li>
</ul>

<p>These might seem pretty obvious recommendations for a distributed system, but <strong>this experiment proves that the underlying design of Dgraph is scalable.</strong></p>

<p>Hope this helps you get a sense of what sort of performance you could expect out of Dgraph!</p>

<p><em>This post is derived from my report for B.tech Project on “A Distributed Implementation of the Graph Database System, Dgraph”.
The full report is <a href="https://www.dropbox.com/s/7h4ytak39r2pdun/Ashwin_Thesis.pdf?dl=0">available for download here</a>.</em></p>

<hr />

<p>If you or your company plan to use a graph database, you can <a href="http://dgraph.io">see our live demo here</a>.
If deep and complex open source distributed systems interest you, <a href="http://dgraph.io">join us. We are hiring!</a></p>

<p>Check out our <a href="https://github.com/dgraph-io/dgraph">Github repository</a>.
Then find us on <a href="https://discuss.dgraph.io">discuss.dgraph.io</a> or <a href="http://slack.dgraph.io">Slack</a>, and come talk to us.</p>

<p><em>- Written by <a href="https://twitter.com/ashwin_rrr">Ashwin Ramesh</a>. Edited by <a href="https://twitter.com/manishrjain">Manish Jain</a></em>.</p>

<p><em>Top image: <a href="http://mars.nasa.gov/images/PIA14840.jpg">Mars Rover Landing via Nasa</a></em></p>

        </div>


        <aside>
          

          <div class="section share">
            <a href="http://www.facebook.com/sharer.php?src=bm&u=http%3a%2f%2fblog.dgraph.io%2fpost%2fperformance-throughput-latency%2f&t=Can%20it%20really%20scale%3f" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-facebook"></i></a>
            <a href="http://twitter.com/intent/tweet?url=http%3a%2f%2fblog.dgraph.io%2fpost%2fperformance-throughput-latency%2f&text=Can%20it%20really%20scale%3f&tw_p=tweetbutton" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-twitter"></i></a>
            <a href="https://plus.google.com/share?url=http%3a%2f%2fblog.dgraph.io%2fpost%2fperformance-throughput-latency%2f" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-google-plus"></i></a>
            <a href="http://getpocket.com/edit?url=http%3a%2f%2fblog.dgraph.io%2fpost%2fperformance-throughput-latency%2f&title=Can%20it%20really%20scale%3f" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-get-pocket"></i></a>
          </div>

          
          
          

					<div id='discourse-comments'></div>
					<script type="text/javascript">
						DiscourseEmbed = { discourseUrl: 'https://discuss.dgraph.io/',
															 discourseEmbedUrl: 'http:\/\/blog.dgraph.io\/post\/performance-throughput-latency\/' };

						(function() {
							var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
							d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js';
							(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
						})();
					</script>
					<noscript>Please enable JavaScript to view the comments on <a href="discuss.dgraph.io">discuss</a>.</noscript>

        </aside>

      </article>
    </div>

  </div>

      </div>

    <footer class="site">
      <p>Copyright (c) 2016, Dgraph Labs, Inc. All rights reserved.</p>
    </footer>

    <script src="//code.jquery.com/jquery-2.1.3.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-75364122-1', 'auto');
    ga('send', 'pageview');
    </script>
    

		<script>
			$(document.links).filter(function() {
						return this.hostname != window.location.hostname;
			}).attr('target', '_blank');
		</script>
  </body>
</html>

</div>
